{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69526049-1e7f-42f7-9763-310d32b3421f",
   "metadata": {},
   "source": [
    "# Text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e26c09-5eb6-49c7-be83-bfc7075170dc",
   "metadata": {},
   "source": [
    "This notebook contains code for the  analysis of text data for the project: Public attitudes towards social media field experiments. Note that all the results were not included in the final manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e7883-e873-4827-9f10-dce45e50d749",
   "metadata": {},
   "source": [
    "## Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387bfd43-8428-43e9-952d-12a68661bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib   # Change cwd\n",
    "import os \n",
    "\n",
    "path = pathlib.Path.cwd().parent\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d0619-8962-416b-ac6d-c721266174c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfdf6f36-c1ab-43bc-8a16-309525fe3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml   # 3rd party packages\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from digex_src import config    # Local imports\n",
    "from digex_src import preprocess\n",
    "from digex_src.load_data import get_data_filepath\n",
    "\n",
    "warnings.filterwarnings('ignore')    # Ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52669183-cf45-473f-b018-054807a0a2d2",
   "metadata": {},
   "source": [
    "## Plotting presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa0adef-ff77-4220-9573-c80c63e8b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default style\n",
    "digex_style = config.MPL_STYLE_FILEPATH\n",
    "digex_palette = config.PALETTE\n",
    "\n",
    "# set size for inline polots\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "plt.style.use(digex_style)\n",
    "sns.color_palette(digex_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7cd644-aa2d-43e4-a3d4-305843a5e190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df524a-2f30-4f1e-8d4a-05598eae8997",
   "metadata": {},
   "source": [
    "### 1. With config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d9b182-ea8f-41d9-90ac-d6bd849c15a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>finished</th>\n",
       "      <th>sm_use</th>\n",
       "      <th>age</th>\n",
       "      <th>gender_id</th>\n",
       "      <th>ethnic_id</th>\n",
       "      <th>edu</th>\n",
       "      <th>politic_views</th>\n",
       "      <th>aware_sm_res</th>\n",
       "      <th>aware_sm_advan</th>\n",
       "      <th>...</th>\n",
       "      <th>rank_pub_interst</th>\n",
       "      <th>rank_add_fac_1</th>\n",
       "      <th>rank_add_fac_1_pos</th>\n",
       "      <th>rank_add_fac_2</th>\n",
       "      <th>rank_add_fac_2_pos</th>\n",
       "      <th>rank_add_fac_3</th>\n",
       "      <th>rank_add_fac_3_pos</th>\n",
       "      <th>aware_sm_advan_score</th>\n",
       "      <th>aware_sm_interact_score</th>\n",
       "      <th>aware_sm_use_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>912.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian - Eastern</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Slightly liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mixed race</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Neutral/ Neither conservative or liberal</td>\n",
       "      <td>Moderately aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1874.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Pacific Islander</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>Very liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1264.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Slightly conservative</td>\n",
       "      <td>Moderately aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>556.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native-American</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Very liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… often capture social relationships not fou...</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration_sec  finished    sm_use   age gender_id          ethnic_id  \\\n",
       "1         912.0      True  Facebook  29.0      Male    Asian - Eastern   \n",
       "2         720.0      True   Twitter  33.0      Male         Mixed race   \n",
       "3        1874.0      True  Facebook  33.0    Female   Pacific Islander   \n",
       "4        1264.0      True  Facebook  73.0    Female  White / Caucasian   \n",
       "5         556.0      True   Twitter  27.0    Female    Native-American   \n",
       "\n",
       "                 edu                             politic_views  \\\n",
       "1         Highschool                          Slightly liberal   \n",
       "2         Highschool  Neutral/ Neither conservative or liberal   \n",
       "3  Bachelor's degree                              Very liberal   \n",
       "4         Highschool                     Slightly conservative   \n",
       "5         Highschool                              Very liberal   \n",
       "\n",
       "       aware_sm_res                                     aware_sm_advan  ...  \\\n",
       "1   Extremely aware  ['… are large and can contain millions of data...  ...   \n",
       "2  Moderately aware  ['… are large and can contain millions of data...  ...   \n",
       "3   Extremely aware  ['… are large and can contain millions of data...  ...   \n",
       "4  Moderately aware  ['… are large and can contain millions of data...  ...   \n",
       "5   Extremely aware  ['… often capture social relationships not fou...  ...   \n",
       "\n",
       "  rank_pub_interst rank_add_fac_1 rank_add_fac_1_pos rank_add_fac_2  \\\n",
       "1              1.0            NaN                NaN            NaN   \n",
       "2              4.0            NaN                NaN            NaN   \n",
       "3              1.0            NaN                NaN            NaN   \n",
       "4              1.0            NaN                8.0            NaN   \n",
       "5              7.0            NaN                NaN            NaN   \n",
       "\n",
       "  rank_add_fac_2_pos rank_add_fac_3 rank_add_fac_3_pos aware_sm_advan_score  \\\n",
       "1                NaN            NaN                NaN                    4   \n",
       "2                NaN            NaN                NaN                    1   \n",
       "3                NaN            NaN                NaN                    2   \n",
       "4                NaN            NaN                NaN                    1   \n",
       "5                NaN            NaN                NaN                    0   \n",
       "\n",
       "  aware_sm_interact_score aware_sm_use_score  \n",
       "1                       0                  9  \n",
       "2                       1                  9  \n",
       "3                       2                  5  \n",
       "4                       1                  6  \n",
       "5                       3                  9  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_path = get_data_filepath(\n",
    "    file=config.PROCESSED_DATA_FILEPATH, \n",
    "    data_path=config.PROCESSED_DATA_DIR,\n",
    "    main=False\n",
    ") \n",
    "\n",
    "digex_df = pd.read_csv(processed_data_path, index_col=0)\n",
    "\n",
    "digex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fa915-a2e7-4555-8c5c-5381a89db272",
   "metadata": {},
   "source": [
    "### 2. With filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9073a6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>finished</th>\n",
       "      <th>sm_use</th>\n",
       "      <th>age</th>\n",
       "      <th>gender_id</th>\n",
       "      <th>ethnic_id</th>\n",
       "      <th>edu</th>\n",
       "      <th>politic_views</th>\n",
       "      <th>aware_sm_res</th>\n",
       "      <th>aware_sm_advan</th>\n",
       "      <th>...</th>\n",
       "      <th>rank_pub_interst</th>\n",
       "      <th>rank_add_fac_1</th>\n",
       "      <th>rank_add_fac_1_pos</th>\n",
       "      <th>rank_add_fac_2</th>\n",
       "      <th>rank_add_fac_2_pos</th>\n",
       "      <th>rank_add_fac_3</th>\n",
       "      <th>rank_add_fac_3_pos</th>\n",
       "      <th>aware_sm_advan_score</th>\n",
       "      <th>aware_sm_interact_score</th>\n",
       "      <th>aware_sm_use_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>912.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian - Eastern</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Slightly liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mixed race</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Neutral/ Neither conservative or liberal</td>\n",
       "      <td>Moderately aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1874.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Pacific Islander</td>\n",
       "      <td>Bachelor's degree</td>\n",
       "      <td>Very liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1264.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>White / Caucasian</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Slightly conservative</td>\n",
       "      <td>Moderately aware</td>\n",
       "      <td>['… are large and can contain millions of data...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>556.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Native-American</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Very liberal</td>\n",
       "      <td>Extremely aware</td>\n",
       "      <td>['… often capture social relationships not fou...</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration_sec  finished    sm_use   age gender_id          ethnic_id  \\\n",
       "1         912.0      True  Facebook  29.0      Male    Asian - Eastern   \n",
       "2         720.0      True   Twitter  33.0      Male         Mixed race   \n",
       "3        1874.0      True  Facebook  33.0    Female   Pacific Islander   \n",
       "4        1264.0      True  Facebook  73.0    Female  White / Caucasian   \n",
       "5         556.0      True   Twitter  27.0    Female    Native-American   \n",
       "\n",
       "                 edu                             politic_views  \\\n",
       "1         Highschool                          Slightly liberal   \n",
       "2         Highschool  Neutral/ Neither conservative or liberal   \n",
       "3  Bachelor's degree                              Very liberal   \n",
       "4         Highschool                     Slightly conservative   \n",
       "5         Highschool                              Very liberal   \n",
       "\n",
       "       aware_sm_res                                     aware_sm_advan  ...  \\\n",
       "1   Extremely aware  ['… are large and can contain millions of data...  ...   \n",
       "2  Moderately aware  ['… are large and can contain millions of data...  ...   \n",
       "3   Extremely aware  ['… are large and can contain millions of data...  ...   \n",
       "4  Moderately aware  ['… are large and can contain millions of data...  ...   \n",
       "5   Extremely aware  ['… often capture social relationships not fou...  ...   \n",
       "\n",
       "  rank_pub_interst rank_add_fac_1 rank_add_fac_1_pos rank_add_fac_2  \\\n",
       "1              1.0            NaN                NaN            NaN   \n",
       "2              4.0            NaN                NaN            NaN   \n",
       "3              1.0            NaN                NaN            NaN   \n",
       "4              1.0            NaN                8.0            NaN   \n",
       "5              7.0            NaN                NaN            NaN   \n",
       "\n",
       "  rank_add_fac_2_pos rank_add_fac_3 rank_add_fac_3_pos aware_sm_advan_score  \\\n",
       "1                NaN            NaN                NaN                    4   \n",
       "2                NaN            NaN                NaN                    1   \n",
       "3                NaN            NaN                NaN                    2   \n",
       "4                NaN            NaN                NaN                    1   \n",
       "5                NaN            NaN                NaN                    0   \n",
       "\n",
       "  aware_sm_interact_score aware_sm_use_score  \n",
       "1                       0                  9  \n",
       "2                       1                  9  \n",
       "3                       2                  5  \n",
       "4                       1                  6  \n",
       "5                       3                  9  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digex_df = pd.read_csv('/Users/jasonburton/Documents/GitHub/article-digex-survey/data/processed/digex-survey-responses-processed.csv', index_col=0)\n",
    "\n",
    "digex_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ec59c-e373-4877-b9cd-885960850c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20eb82-ec57-402b-ac8e-51c1b5c63712",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- see `docs/resources/`\n",
    "- Text as Data, Chris Bail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b73b1-72f9-4589-a8a5-4d3940ec258e",
   "metadata": {},
   "source": [
    "**Variables to examine: 15, 17, 18, 20, 21, 23, 24, 26, 27, 37, 45-50**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914667c",
   "metadata": {},
   "source": [
    "#### Variable 14: understanding of ethical approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c78254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      The scope of the project and actions there in ...\n",
       "2      I think Ethical Approval means that the experi...\n",
       "3      Researchers focus on ethical standards towards...\n",
       "4      I would think that using \"ethical approval\" me...\n",
       "5       A set of rules of what to do and what to not do.\n",
       "                             ...                        \n",
       "495    Approval to do any type of thing that might be...\n",
       "496    It has to with researchers taking a mental not...\n",
       "497    I think ethical approval means that institutio...\n",
       "498    I think ethical approval means that the experi...\n",
       "499    I think ethical approval is that an academic e...\n",
       "Name: ethic_appr, Length: 499, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digex_df['ethic_appr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaaab3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ethical', 249), ('approval', 201), ('mean', 194), ('research', 154), ('study', 122), ('data', 109), ('institution', 98), ('would', 94), ('experiment', 91), ('researcher', 86), ('participant', 84), ('think', 78), ('harm', 73), ('standard', 65), ('sure', 56), ('people', 53), ('need', 49), ('way', 47), ('right', 46), ('information', 45), ('method', 44), ('make', 42), ('board', 41), ('used', 37), ('review', 35), ('ensure', 35), ('cause', 31), ('ethic', 31), ('must', 29), ('get', 28), ('use', 27), ('user', 26), ('getting', 26), ('morally', 25), ('collected', 25), ('without', 24), ('moral', 24), ('set', 23), ('believe', 22), ('violate', 21)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Make all the text lowercase\n",
    "digex_df['ethic_appr'] = digex_df['ethic_appr'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "digex_df['ethic_appr'] = digex_df['ethic_appr'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "digex_df['ethic_appr'] = digex_df['ethic_appr'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "digex_df['ethic_appr'] = digex_df['ethic_appr'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Get the top 20 most frequent words\n",
    "word_counts = Counter(' '.join(digex_df['ethic_appr']).split()).most_common(40)\n",
    "\n",
    "# Print the top 20 most frequent words\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47211818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ethical', 'approval') 135\n",
      "('approval', 'mean') 78\n",
      "('make', 'sure') 36\n",
      "('think', 'ethical') 34\n",
      "('ethical', 'standard') 26\n",
      "('think', 'mean') 22\n",
      "('mean', 'researcher') 18\n",
      "('cause', 'harm') 17\n",
      "('harm', 'participant') 17\n",
      "('mean', 'institution') 16\n",
      "('research', 'study') 16\n",
      "('data', 'collected') 15\n",
      "('review', 'board') 13\n",
      "('approval', 'would') 13\n",
      "('making', 'sure') 12\n",
      "('mean', 'research') 12\n",
      "('getting', 'approval') 11\n",
      "('mean', 'experiment') 10\n",
      "('would', 'think') 10\n",
      "('social', 'medium') 10\n"
     ]
    }
   ],
   "source": [
    "# Get the top 20 most frequent bigrams\n",
    "bigrams = nltk.ngrams(nltk.word_tokenize(' '.join(digex_df['ethic_appr'])), 2)\n",
    "bigram_counts = Counter(bigrams).most_common(20)\n",
    "\n",
    "# Print the top 20 most frequent bigrams\n",
    "for bigram, count in bigram_counts:\n",
    "    print(bigram, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b78202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ethical', 'approval', 'mean') 76\n",
      "('think', 'ethical', 'approval') 33\n",
      "('ethical', 'approval', 'would') 12\n",
      "('institutional', 'review', 'board') 9\n",
      "('approval', 'mean', 'researcher') 9\n",
      "('accepted', 'ethical', 'standard') 7\n",
      "('approval', 'mean', 'getting') 7\n",
      "('approval', 'mean', 'experiment') 6\n",
      "('know', 'access', 'data') 6\n",
      "('participant', 'ethical', 'approval') 6\n",
      "('ethical', 'standard', 'set') 6\n",
      "('make', 'sure', 'experiment') 6\n",
      "('right', 'know', 'access') 5\n",
      "('approval', 'mean', 'institution') 5\n",
      "('ethical', 'standard', 'genuine') 5\n",
      "('standard', 'genuine', 'research') 5\n",
      "('genuine', 'research', 'study') 5\n",
      "('dignity', 'right', 'safety') 5\n",
      "('mean', 'getting', 'approval') 5\n",
      "('researcher', 'participant', 'research') 5\n"
     ]
    }
   ],
   "source": [
    "# Get the top 20 most frequent trigrams\n",
    "trigrams = nltk.ngrams(nltk.word_tokenize(' '.join(digex_df['ethic_appr'])), 3)\n",
    "trigram_counts = Counter(trigrams).most_common(20)\n",
    "\n",
    "# Print the top 20 most frequent trigrams\n",
    "for trigram, count in trigram_counts:\n",
    "    print(trigram, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d9d1c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ethical', 'approval', 'mean', 'research', 'study', 'data', 'institution', 'experiment', 'participant', 'harm', 'would', 'think', 'researcher', 'standard', 'sure', 'right', 'way', 'people', 'method', 'need', 'make', 'cause', 'used', 'morally', 'information', 'board', 'ensure', 'getting', 'ethic', 'moral', 'get', 'permission', 'use', 'review', 'must', 'user', 'done', 'something', 'violate', 'believe']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the text data and transform it to a sparse matrix\n",
    "tfidf = vectorizer.fit_transform(digex_df['ethic_appr'])\n",
    "\n",
    "# Get the names of the features (i.e., the words)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping the feature names to their indices\n",
    "feature_index = {feature: index for index, feature in enumerate(features)}\n",
    "\n",
    "# Sort the feature names by their TF-IDF values\n",
    "sorted_features = sorted(features, key=lambda feature: tfidf[:, feature_index[feature]].sum(), reverse=True)\n",
    "\n",
    "# Print the top 20 features (i.e., words) with the highest TF-IDF values\n",
    "print(sorted_features[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bdb41ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.079*\"study\" + 0.057*\"institution\" + 0.056*\"would\" + 0.048*\"standard\" + 0.040*\"need\" + 0.033*\"think\" + 0.025*\"experiment\" + 0.023*\"participant\" + 0.021*\"moral\" + 0.020*\"right\"')\n",
      "(1, '0.051*\"harm\" + 0.051*\"sure\" + 0.043*\"data\" + 0.040*\"experiment\" + 0.038*\"make\" + 0.036*\"information\" + 0.035*\"people\" + 0.032*\"participant\" + 0.028*\"researcher\" + 0.026*\"standard\"')\n",
      "(2, '0.063*\"data\" + 0.049*\"researcher\" + 0.038*\"experiment\" + 0.038*\"participant\" + 0.036*\"study\" + 0.036*\"institution\" + 0.036*\"method\" + 0.031*\"would\" + 0.030*\"think\" + 0.025*\"review\"')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "digex_df['ethic_appr'] = digex_df['ethic_appr'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Tokenize the text\n",
    "texts = [nltk.word_tokenize(text) for text in digex_df['ethic_appr']]\n",
    "\n",
    "# Create a dictionary mapping words to their frequency\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter out the most frequent words\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "\n",
    "# Create a corpus (a list of bags of words)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create an LDA model\n",
    "model = models.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "\n",
    "# Print the 3 topics\n",
    "for topic in model.show_topics(formatted=True, num_topics=3):\n",
    "    print(topic)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
